---
title: "Spotify Data: An Analysis on Track Popularity"
subtitle: |
  | Final Project 
  | Data Science 1 with R (STAT 301-2)
author: "Olivia Joung"
date: today

format:
  html:
    toc: true
    embed-resources: true
    code-fold: show
    link-external-newwindow: true

execute:
  warning: false
  
from: markdown+emoji 

---
::: {.callout-tip icon=false}

## Github Repo Link

[https://github.com/olivia-joung/final-project-2-olivia-joung.git](https://github.com/olivia-joung/final-project-2-olivia-joung.git)

:::

## Introduction

`spotify_data` is a dataset of 33,000 songs available on Spotify, detailing each track's individual qualitative and quantitative characters (ie music composition, genre, artist, etc.). Additionally, rather than record songs' popularity by its number of streams or something more easily quantifiable, this dataset tracks popularity by a scale from 1-100. (See source information in 'References' below.)

Going into this analysis, I was largely interested in exploring:

1) How accurately the popularity of a song can be predicted just by looking at its quantifiable/identifiable characteristics
2) Which of those characteristics make the best predictors when it comes to approximating popularity

## Data overview and quality

Again, we will use one dataset `spotify_data`. This is a cleaned dataset derived from the raw data `spotify_songs`.

The 16 variables taken into consideration are as follows: 

- `track_popularity` and `track_popularity_bins` (a factor mutation of the original variable `track_popularity` that splits the points up into ranges of 10)
- `playlist_genre` and `playlist_subgenre`
- Remaining quantitative variables that inspect composition of song: `danceability`, `energy`, `key`, `loudness`, `mode`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, and `duration_ms` (aka duration of song in milliseconds)

Variables from the original sets that were not ultimately used in my data analysis can be found in the appendix.

![](plots/track_popularity_bar.png)

Looking closer at `track_popularity`, there are 0 missing values and no clear outliers to take into account (which makes sense, considering all values just range from 0-100).

After splitting the popularity scores into ranges of 10 and plotting, it's clear that most scores range from 40-70, however the range of scores with the highest frequency is 0-10. With that said, looking at the distribution—as well as the mean and median of the data both being around 43-45 (ie closer to the middle)—it appears that the distribution of scores here is normally distributed enough to not require logarithmic transformations when fitting models. 

Looking closer at the data (as seein in `prelim_analysis` R script), the variables that appear to have the highest correlation with `track_popularity` is danceability, energy, speechiness, acousticness, instrumentalness, liveness, tempo, valence, loudness, and duration_ms. Key and mode showed little to no correlation and therefore will be excluded.

Additionally, playlist genre and subgenre also appear to be worth looking more into as predictors, due to the wide variation in track popularity, as seen below:

![](plots/subgenre_boxplot.png)

## Methods

**Target variable is `track_popularity`, with RMSE as my assessment metric. This is a regression problem.**

The main goal here is to compare different models for predicting the popularity of a certain track and adjusting recipes/feature engineering accordingly. 

### Data splitting and resampling

Initial split with a training/testing proportion of 80/20. V-fold cross-validation also used with a v-value of 10 and 5 repeats (full code of initial split seen in corresponding R script). 

### Model types

In addition to the baseline model, we'll also be using:

- Linear regression
- Elastic net 
- Boosted tree
- K-nearest neighbors
- Random forest

### Recipe types

**Basic recipe:**

- Predicted the target variable with all other variables (except `track_popularity_bins`)
- One-hot encoded all categorical predictors
- Filtered out variables have have zero variance
- Centered and scaled all predictors

**Recipe for linear models with all variables:**

- Same steps as basic recipe 
- Also removed key and mode
- Set interactions between tempo and danceability, tempo and energy, energy and loudness, and speechiness and instrumentalness

**Recipe for linear models without playlist genre information:**

- Same steps as first linear model
- Also removed playlist genre variables

**Recipe for tree models with all variables:**

- Same steps as basic recipe 
- Also removed key and mode

**Recipe for tree models without playlist genre information:**

- Same steps as first tree recipe
- Also removed playlist genre variables

## Model Building & Selection Results 
**NOTE TO GRADER: I wasn't able to get my random forest to stop running, however I have still kept my R script with all the tuning information I had.**

**Again: The metric we're using here is RMSE.**


| Model Type          | Best RMSE Mean  | Corresponding Standard Error  | 
|---------------------|-----------------:|-----------------:|
| Baseline        |        22.9          |          0.0251          |
| Linear (w/ basic recipe)         |        22.9          |            0.0251          |
| Linear         |        22.9          |                   0.0249          |
| Linear (no genre) |        24.0          |             0.0239          |         
| K-nearest neighbors (w/ basic recipe)         |        23.7          |            0.0321          |
| K-nearest neighbors         |        23.6              |        0.0302          |       
| K-nearest neighbors (no genre) |        23.7          |              0.0329          |            
| Elastic net (w/ basic recipe)         |        22.9              |        0.0250          |
| Elastic net         |        24.0          |                0.0239          |   
| Elastic net (no genre) |        24.0          |           0.0239          |                   
| Boosted tree (w/ basic recipe)         |        22.7              |        0.0261          |
| Boosted tree         |        22.7          |                   0.0238          |
| Boosted tree (no genre) |        23.6          |            0.0256          |   
| Random forest (w/ basic recipe)         |        n/a          |            n/a          |
| Random forest        |        n/a          |             n/a          |     
| Random forest (no genre) |        n/a          |           n/a          |

: Best RMSE Means for each Model Type {#tbl-mod-totals-reg .striped .hover}

Most successful tuning parameters for each model type:

- Linear (no tuning parameters), however the most successful model type here was the one with the basic recipe. 
- K-nearest neighbors (with tailored recipe including playlist genres) was the most successful out of the three. For parameters, the most successful was a 'neighbors' value of 15.
- Boosted tree model (with the tailored recipe that includes the playlist genres) appears to be the most successful with tuning parameters of: an mtry of 7, min_n of 40, and a learn_rate of 0.631.
- Elastic net (with the most basic recipe) appears to be the most successful with tuning parameters of: a penalty close to 0 and a mixture of 0.762 .

Therefore, it appears that boosted tree (with the tailored recipe, playlist genres included) has the most successful model with an RMSE value of 22.7 and a standard error of 0.0238.

Note that across all categories, the ones using the recipe that excludes playlist genre have the higher RMSE's and therefore are less effective at predicting than those that include them. This means that playlist genres/subgenres do in fact have somewhat of a relationship with the popularity of a track. 

## Final Model Analysis

Therefore, boosted tree (with tailored recipe) may be the best, though based on how high the RMSE is across all models, it appears that building predictive models for popularity at all is not very effective—especially considering the mean RMSE's are all similar to (or even larger than) the baseline model's. 

The lack of correlation and/or the weak ability to predict track popularity via the utilized predictors can be seen here, in which there is clearly very weak amount of overlap between predicted and actual popularity. 

![](results/spotify_plot.png)

## Conclusion

Ultimately, it appears that track popularity cannot be predicted via the predictors that were given to us in the data set—at least with the specific combinations/interactions between potential predictors we chose. Notably, comparing the models with each other clearly showed that the presence of genre/subgenre in data does make a difference, considering those without that information generally proved to have worse predictions that those that had them. This in tandem with the inability of its combination with other predictors to make good guesses for the popularity of a song tells me that—in contrast to what I had expected going in—the actual minute compositional details of a song (its tempo, energy, etc.) does little in the way of predicting popularity. 

In the future, it would be interesting to look deeper and reevaluate what variables—in tandem with genre/subgenre—would in fact make good predictors to a song. I noticed that most of the variables here focused more on the instrumentality of the song rather than the actual lyrics—maybe it would be more helpful to look at things like word count, the kind of voice who's singing it, how many octaves it is sung in, etc. 

## References
`spotify_data` derived from `spotify_songs.csv`^[citation: [public Kaggle dataset](https://www.kaggle.com/datasets/joebeachcapital/30000-spotify-songs)], which is a dataset of roughly 33,000 songs on Spotify.

## Appendix (variables that weren't used)

track_id, track_name, track_album_id, track_artist, track_album_name,
playlist_id, playlist_name, track_album_release_date, key, mode

